{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPf38DcnzcKotKzEeOhhu2N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshcova/NLP_Workshop/blob/main/03_NLP_Spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text representation in Spacy\n",
        "\n",
        "`spacy` is a popular and highly efficient open-source library for Natural Language Processing (NLP) in Python. It offers capabilities for a wide range of NLP tasks, including tokenization, named entity recognition (NER), part-of-speech tagging (POS) etc.\n",
        "\n",
        "When `spacy` processes text, it converts the text into a Doc object. This Doc object is essentially a sequence of Token objects, which are the fundamental building blocks for all subsequent NLP operations.\n",
        "\n",
        "In NLP, a corpus refers to a large and structured set of texts used for linguistic analysis. A document is an individual text within that corpus. In spaCy, the Doc object represents an individual document that has been processed by the nlp pipeline.\n",
        "\n",
        "Tokens are the smallest units of text obtained after splitting a sentence or phrase. They are typically individual words, punctuation marks, or symbols, separated by whitespace or specific rules defined by the tokenizer."
      ],
      "metadata": {
        "id": "YfgvOTc1tDBn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Oi8jZ1faDT3"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we need to do some downloading, we download Spacy's language and trained pipelines, there are different sizes for English: small (en_core_web_sm), medium (en_core_web_md), large (en_core_web_lg)\n",
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_AA7oqRaren",
        "outputId": "fd410b9f-179a-448b-fa97-390e02a3fd79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-md==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the Spacy pipeline into your environment\n",
        "nlp = spacy.load(\"en_core_web_md\")"
      ],
      "metadata": {
        "id": "3HZCUkJba1dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we specify a string (a sentence in this case) and save into a Doc object which we also parse through the Spacy's nlp pipeline\n",
        "\n",
        "doc = nlp(\"This is a sentence. $20 is the price.\")"
      ],
      "metadata": {
        "id": "EHaDA4_Za301"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can see that some tokens are words, some are not\n",
        "\n",
        "for token in doc:\n",
        "  print(token)"
      ],
      "metadata": {
        "id": "TEhE0jVxcL6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Side note: What you have seen above is a for loop - something that is frquently used in Python as well as other programming languages.\n",
        "# You can see its added value by running the code below:\n",
        "\n",
        "for i in range(1,10):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "C97MRBDJ9Hjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Back to Spacy and the NLP world\n",
        "# Tokens are nice, but what tends to be more important is checking the Part-of-Speech (POS) and the Lemma of the individual token.\n",
        "\n",
        "doc = nlp(\"This is a better sentence.\")\n",
        "for token in doc:\n",
        "    print(token, \" | \", token.pos_, \" | \", token.lemma_)"
      ],
      "metadata": {
        "id": "XHVWuVDtcQOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can also do some Named Entity Recognition (NER), which allows us to systematically identify names, places and organizations\n",
        "\n",
        "doc = nlp(\"Elon Musk bought Twitter for $44 billion.\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
      ],
      "metadata": {
        "id": "LH_RXbWYcS-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can combine Spacy to the powerful pandas library and export the output of our analysis into a pandas datafrme\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = []\n",
        "for ent in doc.ents:\n",
        "    data.append({\n",
        "        \"text\": ent.text,\n",
        "        \"label\": ent.label_,\n",
        "        \"explanation\": spacy.explain(ent.label_)\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# if we want to export it to a file which will appear under the files icon on you left, we can use this command:\n",
        "\n",
        "df.to_csv(\"sample_df.csv\")\n",
        "\n",
        "# This is helpful if we want to use the file locally or send the output to a collaborator"
      ],
      "metadata": {
        "id": "gnAgGvFwcZoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopwords"
      ],
      "metadata": {
        "id": "dctkkw4_fKJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Spacy library offers different stop words list for different languages"
      ],
      "metadata": {
        "id": "HDtSR6kL81U6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "## Below for German\n",
        "#from spacy.lang.de.stop_words import STOP_WORDS"
      ],
      "metadata": {
        "id": "5bPmgSeUfJpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Be careful in using off-the-shelf stop words list. The selection of which stop words are in scope or which ones are not should always be dictated by your research question and the corpora that you are working with."
      ],
      "metadata": {
        "id": "uzqmgliq-IJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"This is a sample sentence, showing off the stop words filtration.\")\n",
        "\n",
        "for token in doc:\n",
        "    if token.is_stop:\n",
        "        print(token.text)"
      ],
      "metadata": {
        "id": "GzthnrNsfR4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's bring it all together using the dataframe that we used yesterday!"
      ],
      "metadata": {
        "id": "Z-ao6aeIIW2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/joshcova/NLP_Workshop/refs/heads/main/data/brexit_data.csv\")"
      ],
      "metadata": {
        "id": "PY0y4eumGCBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_select = df[[\"text\",\"party\"]]"
      ],
      "metadata": {
        "id": "PqnpxoPkGGzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's sample some rows from the dataframe\n",
        "\n",
        "df_select = df_select.sample(100, random_state=1)"
      ],
      "metadata": {
        "id": "BycRDpNZGRys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As it always tends to be a good idea to pre-process our text (see lectures' slides), it is customary to include a user-defined function that allows it to scale up the pre-processing to other use cases."
      ],
      "metadata": {
        "id": "I4dKaOWt-bSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# in this pre-processing function, we exclude the stop words from the in-built STOP_WORDS list of the Spacy library as well as punctuation. Your specific case may very well differ.\n",
        "def preprocess(text):\n",
        "    doc = nlp(text)\n",
        "    no_stop_words = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return no_stop_words"
      ],
      "metadata": {
        "id": "aRvNFbU9fV0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now that we our function we can apply using the handy apply method to our dataframe.\n",
        "# The code below therefore applies the in-built preprocess function on our text variable\n",
        "df_select[\"text_no_stop_words\"] = df_select[\"text\"].apply(preprocess)"
      ],
      "metadata": {
        "id": "buocvQX4GoS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Now e can create a new pre-processing function which does no longer remove stop words and punctuation, but rather is focused on retrieving the POS for every token in our dataframe\n",
        "\n",
        "def pos_tag_text(text):\n",
        "    doc = nlp(text)\n",
        "    return [(token.text, token.pos_, token.tag_) for token in doc]"
      ],
      "metadata": {
        "id": "uXjIvbO1H-e1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_select[\"pos\"] = df_select[\"text\"].apply(pos_tag_text)"
      ],
      "metadata": {
        "id": "_D5IFV9AH_-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment analysis\n",
        "\n",
        "Using NLP to conduct sentiment analysis is a frequent use case for applied text-as-data research in the social sciences. But what is sentiment analysis?\n",
        "\n",
        "While there are different methods, in essence a sentiment analysis aims to gauge the sentiment of a text by using computational methods.\n",
        "\n",
        "The most basic application for sentiment analysis is that of sentiment dictionaries. There are different sentiment dictionaries, which list \"positive\" and \"negative\" words (e.g. Lexicoder Sentiment Dictionary).\n",
        "\n",
        "These are called **word-list based sentiment analysis**\n",
        "\n",
        "Here we will showcase how such a sentiment dictionary analysis works by using the [Afinn sentiment analysis](https://github.com/fnielsen/afinn) and the [VADER](https://vadersentiment.readthedocs.io/en/latest/)  sentiment analysis tool.\n",
        "\n",
        "As these are not very commonly used libraries in Python, we would need to go ahead and `pip install` them"
      ],
      "metadata": {
        "id": "oG7hE0C9Gpt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install Afinn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuqBTxG7Ihs3",
        "outputId": "2e5e541b-bed9-4ddb-994f-6a1b2b754cfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Afinn\n",
            "  Downloading afinn-0.1.tar.gz (52 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/52.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: Afinn\n",
            "  Building wheel for Afinn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Afinn: filename=afinn-0.1-py3-none-any.whl size=53431 sha256=f79560413f68122539a3a2c5f9f9836d58d67337ec411a383cf465e42b964538\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/72/27/74994e77200dae3d6aea2b546264500cee21f738c51241320b\n",
            "Successfully built Afinn\n",
            "Installing collected packages: Afinn\n",
            "Successfully installed Afinn-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from afinn import Afinn"
      ],
      "metadata": {
        "id": "7CbNZ6B-Hfzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some easy texts that help us get an intuition of how sentiment analysis work\n",
        "\n",
        "texts = [\n",
        "    \"This is a good movie\",\n",
        "    \"This is not good at all\",\n",
        "    \"What an awful experience!\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "GHWcGnbRImTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "afinn = Afinn()"
      ],
      "metadata": {
        "id": "fAuNCKsfI1dH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_df = pd.DataFrame(texts, columns=[\"text\"])"
      ],
      "metadata": {
        "id": "7iKvK3u6JM9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Now we apply the afinn function from the Afinn library\n",
        "\n",
        "texts_df[\"polarity_score\"] = texts_df[\"text\"].apply(afinn.score)"
      ],
      "metadata": {
        "id": "IkxbLi1oJWpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## How would you interpret this out?\n",
        "\n",
        "print(texts_df)"
      ],
      "metadata": {
        "id": "g-qF41cyD613"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another commoly used rule-based dictionary especially for social media texts is the [VADER](https://vadersentiment.readthedocs.io/en/latest/) (Valence Aware Dictionary and sEntiment Reasoner) sentiment analysis tool\n",
        "\n",
        "Contrary to word-lists, this is a **rules-based** dictionary. Let's analyze the same texts and see what (if anything) changes."
      ],
      "metadata": {
        "id": "YZ9bbiMMIENN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pip install vaderSentiment"
      ],
      "metadata": {
        "id": "STxbhxraHXVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
      ],
      "metadata": {
        "id": "jUSu9L1vHiFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer = SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "id": "d7ls1VY8HqWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in texts:\n",
        "  score = analyzer.polarity_scores(t)\n",
        "  print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tACvsaMHlwm",
        "outputId": "bdd2f0db-15a2-4a56-ce93-9b5d563c9cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.0, 'neu': 0.58, 'pos': 0.42, 'compound': 0.4404}\n",
            "{'neg': 0.325, 'neu': 0.675, 'pos': 0.0, 'compound': -0.3412}\n",
            "{'neg': 0.523, 'neu': 0.477, 'pos': 0.0, 'compound': -0.5093}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## We can apply the same code as we did above for Afinn\n",
        "\n",
        "df_select[\"sentiment_scores\"] = df_select[\"text\"].apply(analyzer.polarity_scores)"
      ],
      "metadata": {
        "id": "V2jF-t_YMGRs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}