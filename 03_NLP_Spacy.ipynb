{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHvcla6Ovxtia7f/B33sXD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshcova/NLP_Workshop/blob/main/03_NLP_Spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text representation in Spacy\n",
        "\n",
        "`spacy` is a popular and highly efficient open-source library for advanced Natural Language Processing (NLP) in Python. It's designed for production use and offers capabilities for a wide range of NLP tasks, including tokenization, named entity recognition (NER), part-of-speech tagging (POS) etc.\n",
        "\n",
        "When `spacy` processes text, it converts the text into a Doc object. This Doc object is essentially a sequence of Token objects, which are the fundamental building blocks for all subsequent NLP operations. Each Token represents an individual word, punctuation mark, or symbol.\n",
        "\n",
        "In NLP, a corpus refers to a large and structured set of texts used for linguistic analysis. A document is an individual text within that corpus. In spaCy, the Doc object represents an individual document that has been processed by the nlp pipeline.\n",
        "\n",
        "Tokens are the smallest units of text obtained after splitting a sentence or phrase. They are typically individual words, punctuation marks, or symbols, separated by whitespace or specific rules defined by the tokenizer."
      ],
      "metadata": {
        "id": "YfgvOTc1tDBn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7Oi8jZ1faDT3"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we need to do some downloading, we download Spacy's language and trained pipelines, there are different sizes for English: small (en_core_web_sm), medium (en_core_web_md), large (en_core_web_lg)\n",
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_AA7oqRaren",
        "outputId": "fd410b9f-179a-448b-fa97-390e02a3fd79"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-md==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the Spacy pipeline into your environment\n",
        "nlp = spacy.load(\"en_core_web_md\")"
      ],
      "metadata": {
        "id": "3HZCUkJba1dk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"This is a sentence. $20 is the price.\")"
      ],
      "metadata": {
        "id": "EHaDA4_Za301"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can see that some tokens are words, some are not\n",
        "\n",
        "for token in doc:\n",
        "  print(token)"
      ],
      "metadata": {
        "id": "TEhE0jVxcL6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can check the Part-of-Speech (POS) and the Lemma\n",
        "\n",
        "doc = nlp(\"This is a better sentence.\")\n",
        "for token in doc:\n",
        "    print(token, \" | \", token.pos_, \" | \", token.lemma_)"
      ],
      "metadata": {
        "id": "XHVWuVDtcQOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can also do some Named Entity Recognition (NER)\n",
        "\n",
        "doc = nlp(\"Elon Musk bought Twitter for $44 billion.\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
      ],
      "metadata": {
        "id": "LH_RXbWYcS-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# export to csv\n",
        "\n",
        "data = []\n",
        "for ent in doc.ents:\n",
        "    data.append({\n",
        "        \"text\": ent.text,\n",
        "        \"label\": ent.label_,\n",
        "        \"explanation\": spacy.explain(ent.label_)\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "gnAgGvFwcZoN"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopwords"
      ],
      "metadata": {
        "id": "dctkkw4_fKJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Spacy library offers different stop words list for different languages"
      ],
      "metadata": {
        "id": "HDtSR6kL81U6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "#from spacy.lang.de.stop_words import STOP_WORDS"
      ],
      "metadata": {
        "id": "5bPmgSeUfJpG"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"This is a sample sentence, showing off the stop words filtration.\")\n",
        "\n",
        "for token in doc:\n",
        "    if token.is_stop:\n",
        "        print(token.text)"
      ],
      "metadata": {
        "id": "GzthnrNsfR4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's bring it all together using the dataframe that we used yesterday!"
      ],
      "metadata": {
        "id": "Z-ao6aeIIW2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/joshcova/NLP_Workshop/refs/heads/main/data/brexit_data.csv\")"
      ],
      "metadata": {
        "id": "PY0y4eumGCBd"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_select = df[[\"text\",\"party\"]]"
      ],
      "metadata": {
        "id": "PqnpxoPkGGzt"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's sample some rows from the dataframe\n",
        "\n",
        "df_select = df_select.sample(100, random_state=1)"
      ],
      "metadata": {
        "id": "BycRDpNZGRys"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    doc = nlp(text)\n",
        "    no_stop_words = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return no_stop_words"
      ],
      "metadata": {
        "id": "aRvNFbU9fV0l"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_select[\"text_no_stop_words\"] = df_select[\"text\"].apply(preprocess)"
      ],
      "metadata": {
        "id": "buocvQX4GoS1"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pos_tag_text(text):\n",
        "    doc = nlp(text)\n",
        "    return [(token.text, token.pos_, token.tag_) for token in doc]"
      ],
      "metadata": {
        "id": "uXjIvbO1H-e1"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_select[\"pos\"] = df_select[\"text\"].apply(pos_tag_text)"
      ],
      "metadata": {
        "id": "_D5IFV9AH_-E"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment analysis\n",
        "\n",
        "There are different sentiment dictionaries, which list \"positive\" and \"negative\" words (e.g. Lexicoder Sentiment Dictionary).\n",
        "\n",
        "These are called **word-list based sentiment analysis**\n",
        "\n",
        "Here we will showcase how such a sentiment dictionary analysis works by using the [Afinn sentiment analysis](https://github.com/fnielsen/afinn) and the [VADER](https://vadersentiment.readthedocs.io/en/latest/)  sentiment analysis tool"
      ],
      "metadata": {
        "id": "oG7hE0C9Gpt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install Afinn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuqBTxG7Ihs3",
        "outputId": "2e5e541b-bed9-4ddb-994f-6a1b2b754cfd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Afinn\n",
            "  Downloading afinn-0.1.tar.gz (52 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/52.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: Afinn\n",
            "  Building wheel for Afinn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Afinn: filename=afinn-0.1-py3-none-any.whl size=53431 sha256=f79560413f68122539a3a2c5f9f9836d58d67337ec411a383cf465e42b964538\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/72/27/74994e77200dae3d6aea2b546264500cee21f738c51241320b\n",
            "Successfully built Afinn\n",
            "Installing collected packages: Afinn\n",
            "Successfully installed Afinn-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from afinn import Afinn"
      ],
      "metadata": {
        "id": "7CbNZ6B-Hfzp"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "    \"This is a good movie\",\n",
        "    \"This is not good at all\",\n",
        "    \"What an awful experience!\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "GHWcGnbRImTo"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "afinn = Afinn()"
      ],
      "metadata": {
        "id": "fAuNCKsfI1dH"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_df = pd.DataFrame(texts, columns=[\"text\"])"
      ],
      "metadata": {
        "id": "7iKvK3u6JM9m"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_df[\"polarity_score\"] = texts_df[\"text\"].apply(afinn.score)"
      ],
      "metadata": {
        "id": "IkxbLi1oJWpL"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another commoly used rule-based dictionary especially for social media texts is the [VADER](https://vadersentiment.readthedocs.io/en/latest/) (Valence Aware Dictionary and sEntiment Reasoner) sentiment analysis tool"
      ],
      "metadata": {
        "id": "YZ9bbiMMIENN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pip install vaderSentiment"
      ],
      "metadata": {
        "id": "STxbhxraHXVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
      ],
      "metadata": {
        "id": "jUSu9L1vHiFa"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer = SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "id": "d7ls1VY8HqWp"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in texts:\n",
        "  score = analyzer.polarity_scores(t)\n",
        "  print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tACvsaMHlwm",
        "outputId": "bdd2f0db-15a2-4a56-ce93-9b5d563c9cb3"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.0, 'neu': 0.58, 'pos': 0.42, 'compound': 0.4404}\n",
            "{'neg': 0.325, 'neu': 0.675, 'pos': 0.0, 'compound': -0.3412}\n",
            "{'neg': 0.523, 'neu': 0.477, 'pos': 0.0, 'compound': -0.5093}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_select[\"sentiment_scores\"] = df_select[\"text\"].apply(analyzer.polarity_scores)"
      ],
      "metadata": {
        "id": "V2jF-t_YMGRs"
      },
      "execution_count": 45,
      "outputs": []
    }
  ]
}