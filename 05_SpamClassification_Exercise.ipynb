{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOal7AVtTtD9WKbklGA7uc9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshcova/NLP_Workshop/blob/main/05_SpamClassification_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spam classification exercise\n",
        "\n",
        "A paradigmatic use case of supervised machine learning models for text classification is that of spam detection.\n",
        "\n",
        "We all are very good in identifying whether an email is *spam* or not (a.k.a. *ham*) - it would helpful however to automate the systematic detection of spam to a machine, which will automatically identify whether a text is spam or not.\n",
        "\n",
        "How do we do it?\n",
        "\n",
        "The model needs to learn what is spam and what is not based on a sample of texts(**training dataset**). Based on this *learning*, it can scale up this classification to \"unseen\" emails on which it had not been trained on (the **testing dataset**).\n",
        "\n",
        "Based on the machine learning text classifier's performance on the unseen, testing dataset we can gauge how well the classifier may perform in classifying whether emails are spam or not in general.\n",
        "\n",
        "Let us see how this works in practice and develop our spam detection classifier!"
      ],
      "metadata": {
        "id": "Usv0cAW7LWrd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's load, as usual, the necessary libraries."
      ],
      "metadata": {
        "id": "nzZc7xc7l0SF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S22TC5zTLHwr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/joshcova/NLP_Workshop/refs/heads/main/data/spam_detection_dataset_1.csv\")"
      ],
      "metadata": {
        "id": "0LwTPU3DLUSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "MwcvDMBwLZRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see the data.frame contains a series of emails (in the \"text\" column) and a classification (\"label\") that checks whether they are spam or not.\n",
        "\n",
        "The idea is to split the dataset into a training and testing dataset, but first we need to get an understanding of the class distribution between the two labels.\n"
      ],
      "metadata": {
        "id": "TECVyHaCLXS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"label\"].value_counts()"
      ],
      "metadata": {
        "id": "B-UZmxd2M_8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to vectorize the text, that is to convert the strings into vectors. As we have covered in the lecture there are two main types of vectorization: a count vectorizer and a TF-IDF vectorizer.\n",
        "\n",
        "Let us start with the count vectorizer first."
      ],
      "metadata": {
        "id": "C_THS-joNF8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer_spam = CountVectorizer()"
      ],
      "metadata": {
        "id": "0wT6_WsTNCCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_spam = df[\"label\"]"
      ],
      "metadata": {
        "id": "ySWDUAUWNlLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We split the dataset into the customary 80%-20% split. The X variable here stands for the text and\n",
        "# y is the outcome that we are trying to predict (think dependent variable), that is our text\n",
        "X_train_spam, X_test_spam, y_train_spam, y_test_spam = train_test_split(X_spam, y_spam, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "G2B-EGXYNqgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_spam.shape)\n",
        "print(y_train_spam.shape)\n",
        "print(X_test_spam.shape)\n",
        "print(y_test_spam.shape)"
      ],
      "metadata": {
        "id": "8O86ZzctN4IE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can peruse how the vectorization works in practice by convertizing the count vectorizer into a data.frame\n",
        "x_test_spam_df = pd.DataFrame(X_test_spam.toarray())"
      ],
      "metadata": {
        "id": "c6xKIORzN6Qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Now you can see the power of the Document-Term Matrix. The documents are the rows and the terms are the columns.\n",
        "\n",
        "x_test_spam_df.head()"
      ],
      "metadata": {
        "id": "VbauJzdGOKr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have separated the dataset into a testing and training dataset, we can start fitting our classifier.\n",
        "\n",
        "For this analysis we use the simple Naive Bayes (MultinomialNB) classifier. In essense what happens is that the Naive Bayes classifier updatas the probability that a specific word is in a specific class depending on the prior likelihood that it had observed it to be in that class.\n",
        "\n",
        "In other words if the word \"free\" has previously been found to occur more often in texts that are classified as spam, it attributes a greater probability that it is in the class spam. The classifier conducts this operation on all the words contained in the text of the different emails and computes an aggregate probability."
      ],
      "metadata": {
        "id": "jAKOTG-7One1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_classifier_spam =  MultinomialNB()"
      ],
      "metadata": {
        "id": "xyinQmkcOkmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is where the actual training takes place. If you think about the operation that is going on \"under the hood\", it is amazing how quickly Python computes it.\n",
        "nb_classifier_spam.fit(X_train_spam, y_train_spam)"
      ],
      "metadata": {
        "id": "ZDEOrJ3WQVc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can make predictions based on what the model has learned on the dataset on which the model has not be trained on (i.e. the testing dataset)\n",
        "\n",
        "y_pred_nb = nb_classifier_spam.predict(X_test_spam)"
      ],
      "metadata": {
        "id": "ZxgECh7yQgz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check out the accuracy score\n",
        "\n",
        "accuracy_score(y_test_spam, y_pred_nb)"
      ],
      "metadata": {
        "id": "jeLI_-UAQsGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can also check the precision and recall score, bearing in mind that we have to define what a positive is. Think back to the formula of accuracy, precision and recall to check the\n",
        "# different ways in which the formulas are computed"
      ],
      "metadata": {
        "id": "d1eKtznUQuzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us define the positive to be \"spam\"\n",
        "precision_score(y_test_spam, y_pred_nb, pos_label=\"spam\")\n",
        "recall_score(y_test_spam, y_pred_nb, pos_label=\"spam\")"
      ],
      "metadata": {
        "id": "QP2kRhpFRA6U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}