{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/USERNAME/REPO/blob/main/Exercise_DTM_TFIDF.ipynb)\n",
    "\n",
    "# Coding Exercise: From Text to Matrix\n",
    "\n",
    "## Building Document-Term Matrices and TF-IDF Representations\n",
    "\n",
    "**Workshop**: Quantitative Text Analysis and Natural Language Processing using Python  \n",
    "**Day 2** â€” Bag-of-Words Models\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this exercise, you will be able to:\n",
    "\n",
    "1. Transform text data into numerical representations using `CountVectorizer` and `TfidfVectorizer`\n",
    "2. Understand and interpret the properties of document-term matrices (dimensionality, sparsity)\n",
    "3. Compare raw frequency counts with TF-IDF weighted representations\n",
    "4. Identify distinctive vocabulary associated with different categories of text\n",
    "\n",
    "### The Data\n",
    "\n",
    "We'll work with the same populism dataset from Day 1: annotated sentences from speeches by European populist leaders. Each sentence has been coded for the type of populist rhetoric it represents:\n",
    "\n",
    "| Code | Category | Description |\n",
    "|------|----------|-------------|\n",
    "| 0 | Neutral | No clear populist framing |\n",
    "| 1 | Us vs. Them | Constructing in-group/out-group divisions |\n",
    "| 2 | People-centrism | Appeals to \"the people\" as a unified group |\n",
    "| 3 | Anti-elite | Criticism of elites, establishment, or powerful groups |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the libraries we'll need and load the data.\n",
    "\n",
    "**If running in Google Colab**: The cell below will automatically download the data file from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# For visualisation (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data if running in Google Colab\n",
    "# ============================================\n",
    "# IMPORTANT: Replace USERNAME/REPO with your actual GitHub username and repository name\n",
    "# For example: 'luukschmitz/nlp-workshop'\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "\n",
    "# Check if data file exists locally; if not, download from GitHub\n",
    "data_file = 'populism_annotation_sample.csv'\n",
    "\n",
    "if not os.path.exists(data_file):\n",
    "    print(\"Downloading data from GitHub...\")\n",
    "    !wget -q https://raw.githubusercontent.com/USERNAME/REPO/main/populism_annotation_sample.csv\n",
    "    print(\"Download complete!\")\n",
    "else:\n",
    "    print(\"Data file found locally.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('populism_annotation_sample.csv')\n",
    "\n",
    "# Create readable labels for the populism categories\n",
    "pop_labels = {\n",
    "    0: 'Neutral',\n",
    "    1: 'Us vs. Them',\n",
    "    2: 'People-centrism',\n",
    "    3: 'Anti-elite'\n",
    "}\n",
    "df['pop_label'] = df['pop_code'].map(pop_labels)\n",
    "\n",
    "print(f\"Data loaded: {len(df)} sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Exploring the Data\n",
    "\n",
    "Before we transform text into numbers, we should understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset overview\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of categories\n",
    "print(\"Distribution of populism categories:\")\n",
    "print(df['pop_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which speakers are in the dataset?\n",
    "print(\"Speakers:\")\n",
    "print(df['speaker'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’­ Question 1\n",
    "\n",
    "Look at the distribution of speakers and categories. What potential issues might this create for our analysis? \n",
    "\n",
    "*Hint: Think about what we might actually be capturing when we identify \"distinctive words\" per category.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** *(double-click to edit)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at example sentences from each category\n",
    "print(\"Example sentences per category:\\n\")\n",
    "\n",
    "for code in sorted(df['pop_code'].unique()):\n",
    "    label = pop_labels[code]\n",
    "    example = df[df['pop_code'] == code]['translated_sentence'].iloc[0]\n",
    "    print(f\"--- {label} (code {code}) ---\")\n",
    "    print(f\"{example}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Creating a Count-based Document-Term Matrix\n",
    "\n",
    "The **Document-Term Matrix (DTM)** is the foundational representation for bag-of-words models. Each row represents a document (in our case, a sentence), and each column represents a unique word in the vocabulary. The cells contain word counts.\n",
    "\n",
    "We'll use scikit-learn's `CountVectorizer` to create this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the CountVectorizer with default settings\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform: learn vocabulary from text and create the matrix\n",
    "dtm_count = count_vectorizer.fit_transform(df['translated_sentence'])\n",
    "\n",
    "# Get the vocabulary (feature names)\n",
    "vocabulary = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"DTM shape: {dtm_count.shape}\")\n",
    "print(f\"  - {dtm_count.shape[0]} documents\")\n",
    "print(f\"  - {dtm_count.shape[1]} unique terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’­ Question 2\n",
    "\n",
    "The DTM has 40 rows and several hundred columns. In plain language, what does each number in this matrix represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** *(double-click to edit)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's peek at the vocabulary\n",
    "print(\"First 20 terms (alphabetically sorted):\")\n",
    "print(vocabulary[:20])\n",
    "\n",
    "print(\"\\nLast 20 terms:\")\n",
    "print(vocabulary[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DTM is stored as a \"sparse matrix\" for efficiency\n",
    "# Let's convert a small portion to see what it looks like\n",
    "\n",
    "# Convert to a regular DataFrame for the first 5 documents and first 10 words\n",
    "sample_dtm = pd.DataFrame(\n",
    "    dtm_count[:5, :10].toarray(),  # First 5 docs, first 10 terms\n",
    "    columns=vocabulary[:10]\n",
    ")\n",
    "\n",
    "print(\"Sample of the DTM (first 5 documents, first 10 terms):\")\n",
    "print(sample_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Creating a TF-IDF Matrix\n",
    "\n",
    "Raw counts treat all words equally, but not all words are equally informative. **TF-IDF** (Term Frequencyâ€“Inverse Document Frequency) reweights words by how distinctive they are.\n",
    "\n",
    "$$\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)$$\n",
    "\n",
    "- **TF** (Term Frequency): How often does word $t$ appear in document $d$?\n",
    "- **IDF** (Inverse Document Frequency): How rare is word $t$ across all documents?\n",
    "\n",
    "Words that appear frequently in one document but rarely across the corpus get high TF-IDF scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform\n",
    "dtm_tfidf = tfidf_vectorizer.fit_transform(df['translated_sentence'])\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {dtm_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that both vectorizers produce the same vocabulary\n",
    "vocab_count = count_vectorizer.get_feature_names_out()\n",
    "vocab_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"Same vocabulary: {np.array_equal(vocab_count, vocab_tfidf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Examining Vocabulary Size and Sparsity\n",
    "\n",
    "Text data has some distinctive properties. Let's examine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sparsity\n",
    "total_cells = dtm_count.shape[0] * dtm_count.shape[1]\n",
    "non_zero_cells = dtm_count.nnz  # .nnz gives the number of non-zero elements\n",
    "sparsity = 1 - (non_zero_cells / total_cells)\n",
    "\n",
    "print(f\"Matrix dimensions: {dtm_count.shape[0]} documents Ã— {dtm_count.shape[1]} terms\")\n",
    "print(f\"\\nTotal cells: {total_cells:,}\")\n",
    "print(f\"Non-zero cells: {non_zero_cells:,}\")\n",
    "print(f\"Sparsity: {sparsity:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’­ Question 3\n",
    "\n",
    "The matrix is over 95% zeros. Why is this? Is this a problem or a feature of text data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** *(double-click to edit)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many words per document?\n",
    "words_per_doc = np.array(dtm_count.sum(axis=1)).flatten()\n",
    "\n",
    "print(\"Words per document:\")\n",
    "print(f\"  Mean: {words_per_doc.mean():.1f}\")\n",
    "print(f\"  Min:  {words_per_doc.min()}\")\n",
    "print(f\"  Max:  {words_per_doc.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word frequency distribution (Zipf's Law in action)\n",
    "word_frequencies = np.array(dtm_count.sum(axis=0)).flatten()\n",
    "\n",
    "print(\"Word frequency distribution:\")\n",
    "print(f\"  Words appearing exactly once: {np.sum(word_frequencies == 1)}\")\n",
    "print(f\"  Words appearing 2-5 times:    {np.sum((word_frequencies >= 2) & (word_frequencies <= 5))}\")\n",
    "print(f\"  Words appearing 6+ times:     {np.sum(word_frequencies >= 6)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’­ Question 4\n",
    "\n",
    "Most words appear only once (these are called *hapax legomena*). What does this tell us about the challenges of working with small text corpora?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** *(double-click to edit)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the most frequent words?\n",
    "word_freq_df = pd.DataFrame({\n",
    "    'word': vocabulary,\n",
    "    'count': word_frequencies\n",
    "}).sort_values('count', ascending=False)\n",
    "\n",
    "print(\"Top 15 most frequent words:\")\n",
    "print(word_freq_df.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’­ Question 5\n",
    "\n",
    "Look at the most frequent words. How many of these would you consider substantively meaningful for understanding populist rhetoric? What does this suggest about the importance of preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** *(double-click to edit)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Comparing Raw Counts vs. TF-IDF\n",
    "\n",
    "Now let's see how TF-IDF changes which words appear important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert matrices to DataFrames for easier manipulation\n",
    "dtm_count_df = pd.DataFrame(\n",
    "    dtm_count.toarray(),\n",
    "    columns=vocabulary\n",
    ")\n",
    "\n",
    "dtm_tfidf_df = pd.DataFrame(\n",
    "    dtm_tfidf.toarray(),\n",
    "    columns=vocabulary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare common words vs. distinctive words\n",
    "words_to_compare = ['the', 'to', 'of',           # Function words (stopwords)\n",
    "                    'people', 'our', 'we',        # Potentially meaningful\n",
    "                    'corruption', 'freedom']      # Clearly substantive\n",
    "\n",
    "# Filter to words that exist in our vocabulary\n",
    "words_to_compare = [w for w in words_to_compare if w in vocabulary]\n",
    "\n",
    "print(\"Comparison: Raw Counts vs. TF-IDF\\n\")\n",
    "print(f\"{'Word':<12} {'Raw Count':>12} {'Avg TF-IDF':>12} {'Docs':>8}\")\n",
    "print(\"-\" * 48)\n",
    "\n",
    "for word in words_to_compare:\n",
    "    raw_count = dtm_count_df[word].sum()\n",
    "    tfidf_values = dtm_tfidf_df[word]\n",
    "    docs_with_word = (tfidf_values > 0).sum()\n",
    "    avg_tfidf = tfidf_values[tfidf_values > 0].mean() if docs_with_word > 0 else 0\n",
    "    \n",
    "    print(f\"{word:<12} {raw_count:>12} {avg_tfidf:>12.4f} {docs_with_word:>8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’­ Question 6\n",
    "\n",
    "Notice that \"the\" appears 102 times but has a relatively low average TF-IDF score, while \"freedom\" appears only twice but has a high TF-IDF score. Explain why this happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** *(double-click to edit)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a single document in detail\n",
    "doc_index = 0\n",
    "\n",
    "print(f\"Document {doc_index}:\")\n",
    "print(f\"'{df['translated_sentence'].iloc[doc_index]}'\")\n",
    "print(f\"\\nCategory: {df['pop_label'].iloc[doc_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top words in this document by raw count\n",
    "doc_counts = dtm_count_df.iloc[doc_index]\n",
    "doc_tfidf = dtm_tfidf_df.iloc[doc_index]\n",
    "\n",
    "# Only non-zero entries\n",
    "doc_counts_nonzero = doc_counts[doc_counts > 0].sort_values(ascending=False)\n",
    "doc_tfidf_nonzero = doc_tfidf[doc_tfidf > 0].sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 8 words by RAW COUNT:\")\n",
    "for word, count in doc_counts_nonzero.head(8).items():\n",
    "    print(f\"  {word}: {int(count)}\")\n",
    "\n",
    "print(\"\\nTop 8 words by TF-IDF:\")\n",
    "for word, score in doc_tfidf_nonzero.head(8).items():\n",
    "    print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Identifying Distinctive Words per Category\n",
    "\n",
    "Now for the payoff: can we identify which words are most associated with each type of populist rhetoric?\n",
    "\n",
    "**Approach**: For each category, we calculate the mean TF-IDF score for each word across all documents in that category. Words with high mean TF-IDF are distinctive of that category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean TF-IDF per category\n",
    "for code in sorted(df['pop_code'].unique()):\n",
    "    label = pop_labels[code]\n",
    "    \n",
    "    # Select documents from this category\n",
    "    category_mask = df['pop_code'] == code\n",
    "    \n",
    "    # Calculate mean TF-IDF for each word\n",
    "    mean_tfidf = dtm_tfidf_df[category_mask].mean()\n",
    "    \n",
    "    # Get top words\n",
    "    top_words = mean_tfidf.sort_values(ascending=False).head(10)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{label.upper()} (code {code})\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"\\nTop 10 distinctive words:\")\n",
    "    for i, (word, score) in enumerate(top_words.items(), 1):\n",
    "        print(f\"  {i:2}. {word:<15} {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’­ Question 7\n",
    "\n",
    "Look at the distinctive words for each category. Do they make substantive sense given what the categories are supposed to capture? Which category has the most interpretable distinctive vocabulary? Which is least interpretable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** *(double-click to edit)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Two Categories\n",
    "\n",
    "Another way to find distinctive vocabulary is to directly compare categories. Let's compare **Anti-elite** rhetoric with **Neutral** speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean TF-IDF for each category\n",
    "mean_antielite = dtm_tfidf_df[df['pop_code'] == 3].mean()\n",
    "mean_neutral = dtm_tfidf_df[df['pop_code'] == 0].mean()\n",
    "\n",
    "# Difference: positive = more anti-elite, negative = more neutral\n",
    "diff = mean_antielite - mean_neutral\n",
    "\n",
    "print(\"Words MORE associated with Anti-elite rhetoric:\")\n",
    "print(\"-\" * 40)\n",
    "for word, score in diff.sort_values(ascending=False).head(10).items():\n",
    "    print(f\"  {word:<15} +{score:.4f}\")\n",
    "\n",
    "print(\"\\nWords MORE associated with Neutral speech:\")\n",
    "print(\"-\" * 40)\n",
    "for word, score in diff.sort_values(ascending=True).head(10).items():\n",
    "    print(f\"  {word:<15} {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ§ª Your Turn: Hands-on Task\n",
    "\n",
    "Now it's your turn to apply what you've learned. Complete the following task:\n",
    "\n",
    "**Task**: Compare **People-centrism** (code 2) with **Us vs. Them** (code 1). Which words distinguish these two categories?\n",
    "\n",
    "Use the code cell below to:\n",
    "1. Calculate mean TF-IDF for both categories\n",
    "2. Find the difference\n",
    "3. Identify the top 10 words that distinguish each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Hint: Follow the pattern from the Anti-elite vs. Neutral comparison above\n",
    "\n",
    "# Step 1: Calculate mean TF-IDF for People-centrism (code 2)\n",
    "mean_people = ___\n",
    "\n",
    "# Step 2: Calculate mean TF-IDF for Us vs. Them (code 1)\n",
    "mean_usvsthem = ___\n",
    "\n",
    "# Step 3: Calculate the difference\n",
    "diff = ___\n",
    "\n",
    "# Step 4: Print results\n",
    "print(\"Words MORE associated with People-centrism:\")\n",
    "# ...\n",
    "\n",
    "print(\"\\nWords MORE associated with Us vs. Them:\")\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "Before wrapping up, consider these broader questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’­ Question 8\n",
    "\n",
    "We found that function words like \"the\", \"of\", and \"to\" often dominate our results. What preprocessing step could we add to address this? What might we lose if we remove these words entirely?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** *(double-click to edit)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’­ Question 9\n",
    "\n",
    "Our corpus has only 40 sentences. How might our results change with a larger corpus (e.g., 1,000 or 10,000 sentences)? Think about both the vocabulary and the distinctiveness of words per category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** *(double-click to edit)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’­ Question 10\n",
    "\n",
    "We used TF-IDF to identify \"distinctive\" words, but distinctiveness is not the same as importance or meaning. What are the limitations of this approach for understanding populist rhetoric? What would you need to complement this analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** *(double-click to edit)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned to:\n",
    "\n",
    "1. **Transform text to numbers** using `CountVectorizer` (raw counts) and `TfidfVectorizer` (weighted counts)\n",
    "2. **Understand DTM properties**: high dimensionality (one column per word), extreme sparsity (most cells are zero)\n",
    "3. **Compare representations**: TF-IDF downweights common words and highlights distinctive vocabulary\n",
    "4. **Find distinctive words**: By comparing mean TF-IDF across categories\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Text data is inherently high-dimensional and sparse\n",
    "- Raw frequency counts are dominated by common function words\n",
    "- TF-IDF reweighting emphasises words that are frequent *in some documents* but rare *across the corpus*\n",
    "- With small corpora, many words appear only once, limiting statistical power\n",
    "- Bag-of-words representations ignore word order and contextâ€”a significant limitation\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the spring workshop, we'll see how **word embeddings** and **LLMs** address some of these limitations by capturing semantic similarity and context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus: Visualisation (Optional)\n",
    "\n",
    "If time permits, here's a quick visualisation of the distinctive words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple bar chart of top words per category\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, code in enumerate(sorted(df['pop_code'].unique())):\n",
    "    label = pop_labels[code]\n",
    "    category_mask = df['pop_code'] == code\n",
    "    mean_tfidf = dtm_tfidf_df[category_mask].mean()\n",
    "    top_words = mean_tfidf.sort_values(ascending=True).tail(10)  # Ascending for horizontal bar\n",
    "    \n",
    "    axes[idx].barh(top_words.index, top_words.values, color='steelblue')\n",
    "    axes[idx].set_title(f'{label}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Mean TF-IDF')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Top 10 Distinctive Words per Populism Category', y=1.02, fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
